{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115229f7",
   "metadata": {},
   "source": [
    "Welcome to the Python Archeologist project, an expedition into the vast terrains of Natural Language Processing (NLP). As aspiring linguistic excavators, you will embark on a journey to discover obscured words, harnessing the power of the Word2Vec model.\n",
    "\n",
    "As we've discussed in our lectures, in the realm of language, context reigns supreme. Words draw much of their meaning from the surrounding words. Your challenge for this project? Trying to predict words in documents that have been buried thousands of years underground! Our goal is to help archaelogists make sense of certain documents that have words that are ineligible:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9808148a",
   "metadata": {},
   "source": [
    "![image](https://th.bing.com/th/id/OIG._ZvxAQdM.h2kWO.7ONMn?pid=ImgGn&w=1024&h=1024&rs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63ec55",
   "metadata": {},
   "source": [
    "To do that, we'll use some text to train a Word2Vec model that will be able to predict the center word based on context! After developing this model, we'll also be able to extract the latent meaning of our words by accessing the weights of the trained neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6dd323a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries we may need: \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7776c0f",
   "metadata": {},
   "source": [
    "### Project - Predict the Hidden Word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1c14ef",
   "metadata": {},
   "source": [
    "To make sense of the hidden words, we need to train our Word2Vec model first! First, let's load our training base into Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bcd50c",
   "metadata": {},
   "source": [
    "Load the `wiki_pages.txt` file stored in the `data` folder using `python`. \n",
    "<br>\n",
    "*Hint: Watch out for file encoding!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f8d4a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea201477",
   "metadata": {},
   "source": [
    "Remove all punctuation from the file you've just loaded into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53252b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d644a00",
   "metadata": {},
   "source": [
    "Tokenize the file you loaded using `nltk's word tokenize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4262bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d8de33",
   "metadata": {},
   "source": [
    "Lower case all tokens in the tokenized version of the text you've just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "412c9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a64878",
   "metadata": {},
   "source": [
    "Generate the training base for the tokens with a context of two neighbors on each side. For example, for the sentence 'much of Lower Egypt around', the features should be 'much of Egypt around' and the target should be 'lower'. You can use an average of the one-hot-vectors of individual words to generate the array for the context. The array for the target is a one-hot vector representing the target word. \n",
    "<br>\n",
    "<br>\n",
    "*Hint: Check the code of the lectures where we've used wikipedia data!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676d9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375abbe",
   "metadata": {},
   "source": [
    "Split the target and features data intro train and test using 20% of your test set for evaluation of the algorithm (select the test set randomnly).\n",
    "<br>\n",
    "<br>\n",
    "*Hint: Use train_test_split from sklearn!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cdc17d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e7f6a2",
   "metadata": {},
   "source": [
    "Train a cbow model using *keras*. Your word vectors (inner layer) should have a size of 40 dimensions. Use any set of hyperparameters (`epochs, batch size, etc`) as you would like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7559e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ff3a98",
   "metadata": {},
   "source": [
    "The archaelogists found ineligible documents with the following sentences: \n",
    "- `the muhammad ___ dynasty remained`\n",
    "- `because the ___ empire was`\n",
    "- `egypt and ___ formed a`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d822e1",
   "metadata": {},
   "source": [
    "Using the trained machine learning model, try to predict the center words above and complete the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2539a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5be653",
   "metadata": {},
   "source": [
    "Final task! The archaelogists want to understand which words are more similar to `egypt` (top 10) in our word vectors context. \n",
    "<br>\n",
    "<br>\n",
    "Extract the word vectors from our trained model (use any method you would like and from any layer you would want) and check which words are more similar to `egypt` using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41009e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPCourse",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
