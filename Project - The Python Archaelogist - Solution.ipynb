{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115229f7",
   "metadata": {},
   "source": [
    "Welcome to the Python Archeologist project, an expedition into the vast terrains of Natural Language Processing (NLP). As aspiring linguistic excavators, you will embark on a journey to discover obscured words, harnessing the power of the Word2Vec model.\n",
    "\n",
    "As we've discussed in our lectures, in the realm of language, context reigns supreme. Words draw much of their meaning from the surrounding words. Your challenge for this project? Trying to predict words in documents that have been buried thousands of years underground! Our goal is to help archaelogists make sense of certain documents that have words that are ineligible:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9808148a",
   "metadata": {},
   "source": [
    "![image](https://th.bing.com/th/id/OIG._ZvxAQdM.h2kWO.7ONMn?pid=ImgGn&w=1024&h=1024&rs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63ec55",
   "metadata": {},
   "source": [
    "To do that, we'll use some text to train a Word2Vec model that will be able to predict the center word based on context! After developing this model, we'll also be able to extract the latent meaning of our words by accessing the weights of the trained neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6dd323a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries we may need: \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7776c0f",
   "metadata": {},
   "source": [
    "### Project - Predict the Hidden Word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1c14ef",
   "metadata": {},
   "source": [
    "To make sense of the hidden words, we need to train our Word2Vec model first! First, let's load our training base into Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bcd50c",
   "metadata": {},
   "source": [
    "Load the `wiki_pages.txt` file stored in the `data` folder using `python`. \n",
    "<br>\n",
    "*Hint: Watch out for file encoding!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f8d4a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wiki_pages.txt', encoding='UTF-8') as f:\n",
    "    wiki_file = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea201477",
   "metadata": {},
   "source": [
    "Remove all punctuation from the file you've just loaded into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53252b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_file = (\n",
    "    wiki_file\n",
    "    .translate(\n",
    "        str.maketrans('', '', string.punctuation)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d644a00",
   "metadata": {},
   "source": [
    "Tokenize the file you loaded using `nltk's word tokenize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4262bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_archives = word_tokenize(wiki_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d8de33",
   "metadata": {},
   "source": [
    "Lower case all tokens in the tokenized version of the text you've just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "412c9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_archives = [token.lower() for token in token_archives]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a64878",
   "metadata": {},
   "source": [
    "Generate the training base for the tokens with a context of two neighbors on each side. For example, for the sentence 'much of Lower Egypt around', the features should be 'much of Egypt around' and the target should be 'lower'. You can use an average of the one-hot-vectors of individual words to generate the array for the context. The array for the target is a one-hot vector representing the target word. \n",
    "<br>\n",
    "<br>\n",
    "*Hint: Check the code of the lectures where we've used wikipedia data!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "676d9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(\n",
    "    set(token_archives)\n",
    ")\n",
    "\n",
    "vocab.sort()\n",
    "\n",
    "word_representations = np.identity(len(vocab))\n",
    "\n",
    "vocab_vectors = {}\n",
    "\n",
    "for index, element in enumerate(vocab):\n",
    "    vocab_vectors[element] = word_representations[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47fdba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_word_neighbors(sentence, neighbors):\n",
    "    '''\n",
    "    Retrieves word and neighbor(context) of size\n",
    "    neighbors into two separate lists.\n",
    "    \n",
    "    Arguments:\n",
    "    - Sentence(str): The sentence to retrieve\n",
    "    words and context words.\n",
    "    - Neighbors(int): The size of the window to\n",
    "    consider context.\n",
    "    \n",
    "    Returns:\n",
    "    - word_keys_sentence(list): Word list;\n",
    "    - context_words(list): A list with the context\n",
    "    words for each word of index i\n",
    "    '''\n",
    "    word_keys_sentence = []\n",
    "    context_words = []    \n",
    "    \n",
    "    for index, word in enumerate(sentence):\n",
    "        \n",
    "        # Build start\n",
    "        # Build finish\n",
    "        start = index-neighbors        \n",
    "        finish = index+neighbors\n",
    "        \n",
    "        # Get neighbor words\n",
    "        neighbor_words = sentence[start:finish+1]\n",
    "        # Generate context\n",
    "        word_context = (\n",
    "            neighbor_words[:neighbors]\n",
    "            +\n",
    "            neighbor_words[neighbors+1:]\n",
    "        )\n",
    "        \n",
    "        # We only append the context if we have enough \n",
    "        # neighbors\n",
    "        if len(word_context) >= neighbors*2:\n",
    "            word_keys_sentence.append(word)\n",
    "            context_words.append(word_context)\n",
    "            \n",
    "    return word_keys_sentence, context_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94b93e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_keys, context_window = retrieve_word_neighbors(token_archives, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3572a513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "train_word_size = len(word_keys)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "y = np.zeros([train_word_size, vocab_size])\n",
    "X = np.zeros([train_word_size, vocab_size])\n",
    "\n",
    "neighbors = 2\n",
    "for index, word in enumerate(word_keys):\n",
    "    if index % 1000 == 0:\n",
    "        print(index)\n",
    "    y[index,:] = vocab_vectors[word]\n",
    "    aux_array = np.zeros([1, vocab_size])\n",
    "    \n",
    "    for neighbour in context_window[index]:\n",
    "        aux_array = aux_array+vocab_vectors[neighbour]\n",
    "    \n",
    "    X[index,:] = (aux_array/(neighbors*2))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375abbe",
   "metadata": {},
   "source": [
    "Split the target and features data intro train and test using 20% of your test set for evaluation of the algorithm (select the test set randomnly).\n",
    "<br>\n",
    "<br>\n",
    "*Hint: Use train_test_split from sklearn!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cdc17d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose sklearn use train_test_split \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e7f6a2",
   "metadata": {},
   "source": [
    "Train a cbow model using *keras*. Your word vectors (inner layer) should have a size of 40 dimensions. Use any set of hyperparameters (`epochs, batch size, etc`) as you would like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7559e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(40, input_dim=vocab_size, activation='relu'))\n",
    "model.add(Dense(vocab_size, input_dim=40, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c246d3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "542/542 [==============================] - 35s 65ms/step - loss: 7.6781 - accuracy: 0.0824 - val_loss: 7.2738 - val_accuracy: 0.0744\n",
      "Epoch 2/80\n",
      "542/542 [==============================] - 16s 30ms/step - loss: 6.9776 - accuracy: 0.0830 - val_loss: 7.3249 - val_accuracy: 0.0744\n",
      "Epoch 3/80\n",
      "542/542 [==============================] - 20s 36ms/step - loss: 6.9180 - accuracy: 0.0830 - val_loss: 7.3836 - val_accuracy: 0.0744\n",
      "Epoch 4/80\n",
      "542/542 [==============================] - 21s 39ms/step - loss: 6.8757 - accuracy: 0.0853 - val_loss: 7.4032 - val_accuracy: 0.0838\n",
      "Epoch 5/80\n",
      "542/542 [==============================] - 19s 35ms/step - loss: 6.8196 - accuracy: 0.1016 - val_loss: 7.4202 - val_accuracy: 0.0938\n",
      "Epoch 6/80\n",
      "542/542 [==============================] - 19s 35ms/step - loss: 6.7613 - accuracy: 0.1041 - val_loss: 7.4388 - val_accuracy: 0.0953\n",
      "Epoch 7/80\n",
      "542/542 [==============================] - 22s 40ms/step - loss: 6.7088 - accuracy: 0.1067 - val_loss: 7.4577 - val_accuracy: 0.0981\n",
      "Epoch 8/80\n",
      "542/542 [==============================] - 18s 34ms/step - loss: 6.6561 - accuracy: 0.1118 - val_loss: 7.4836 - val_accuracy: 0.1015\n",
      "Epoch 9/80\n",
      "542/542 [==============================] - 19s 35ms/step - loss: 6.6017 - accuracy: 0.1172 - val_loss: 7.5034 - val_accuracy: 0.1035\n",
      "Epoch 10/80\n",
      "542/542 [==============================] - 19s 35ms/step - loss: 6.5426 - accuracy: 0.1218 - val_loss: 7.5393 - val_accuracy: 0.1046\n",
      "Epoch 11/80\n",
      "542/542 [==============================] - 31s 58ms/step - loss: 6.4781 - accuracy: 0.1260 - val_loss: 7.5341 - val_accuracy: 0.1075\n",
      "Epoch 12/80\n",
      "542/542 [==============================] - 34s 63ms/step - loss: 6.4064 - accuracy: 0.1331 - val_loss: 7.5529 - val_accuracy: 0.1131\n",
      "Epoch 13/80\n",
      "542/542 [==============================] - 25s 46ms/step - loss: 6.3269 - accuracy: 0.1414 - val_loss: 7.5622 - val_accuracy: 0.1138\n",
      "Epoch 14/80\n",
      "542/542 [==============================] - 25s 46ms/step - loss: 6.2402 - accuracy: 0.1517 - val_loss: 7.5674 - val_accuracy: 0.1154\n",
      "Epoch 15/80\n",
      "542/542 [==============================] - 18s 33ms/step - loss: 6.1473 - accuracy: 0.1607 - val_loss: 7.5913 - val_accuracy: 0.1161\n",
      "Epoch 16/80\n",
      "542/542 [==============================] - 18s 34ms/step - loss: 6.0491 - accuracy: 0.1706 - val_loss: 7.5964 - val_accuracy: 0.1189\n",
      "Epoch 17/80\n",
      "542/542 [==============================] - 21s 40ms/step - loss: 5.9467 - accuracy: 0.1797 - val_loss: 7.6058 - val_accuracy: 0.1224\n",
      "Epoch 18/80\n",
      "542/542 [==============================] - 18s 34ms/step - loss: 5.8415 - accuracy: 0.1882 - val_loss: 7.6411 - val_accuracy: 0.1239\n",
      "Epoch 19/80\n",
      "542/542 [==============================] - 18s 34ms/step - loss: 5.7343 - accuracy: 0.1959 - val_loss: 7.6554 - val_accuracy: 0.1241\n",
      "Epoch 20/80\n",
      "542/542 [==============================] - 19s 34ms/step - loss: 5.6253 - accuracy: 0.2035 - val_loss: 7.6963 - val_accuracy: 0.1289\n",
      "Epoch 21/80\n",
      "542/542 [==============================] - 19s 35ms/step - loss: 5.5144 - accuracy: 0.2103 - val_loss: 7.7283 - val_accuracy: 0.1299\n",
      "Epoch 22/80\n",
      "542/542 [==============================] - 19s 36ms/step - loss: 5.4023 - accuracy: 0.2167 - val_loss: 7.7787 - val_accuracy: 0.1312\n",
      "Epoch 23/80\n",
      "542/542 [==============================] - 18s 32ms/step - loss: 5.2893 - accuracy: 0.2232 - val_loss: 7.8241 - val_accuracy: 0.1314\n",
      "Epoch 24/80\n",
      "542/542 [==============================] - 19s 35ms/step - loss: 5.1752 - accuracy: 0.2297 - val_loss: 7.8808 - val_accuracy: 0.1334\n",
      "Epoch 25/80\n",
      "542/542 [==============================] - 21s 38ms/step - loss: 5.0603 - accuracy: 0.2354 - val_loss: 7.9267 - val_accuracy: 0.1346\n",
      "Epoch 26/80\n",
      "542/542 [==============================] - 18s 34ms/step - loss: 4.9445 - accuracy: 0.2422 - val_loss: 7.9936 - val_accuracy: 0.1334\n",
      "Epoch 27/80\n",
      "542/542 [==============================] - 18s 34ms/step - loss: 4.8284 - accuracy: 0.2485 - val_loss: 8.0600 - val_accuracy: 0.1346\n",
      "Epoch 28/80\n",
      "542/542 [==============================] - 18s 32ms/step - loss: 4.7114 - accuracy: 0.2547 - val_loss: 8.1430 - val_accuracy: 0.1342\n",
      "Epoch 29/80\n",
      "542/542 [==============================] - 20s 37ms/step - loss: 4.5944 - accuracy: 0.2612 - val_loss: 8.2218 - val_accuracy: 0.1352\n",
      "Epoch 30/80\n",
      "542/542 [==============================] - 18s 34ms/step - loss: 4.4780 - accuracy: 0.2670 - val_loss: 8.3068 - val_accuracy: 0.1367\n",
      "Epoch 31/80\n",
      "542/542 [==============================] - 3782s 7s/step - loss: 4.3618 - accuracy: 0.2733 - val_loss: 8.3935 - val_accuracy: 0.1362\n",
      "Epoch 32/80\n",
      "542/542 [==============================] - 35s 65ms/step - loss: 4.2464 - accuracy: 0.2795 - val_loss: 8.4961 - val_accuracy: 0.1391\n",
      "Epoch 33/80\n",
      "542/542 [==============================] - 33s 61ms/step - loss: 4.1320 - accuracy: 0.2873 - val_loss: 8.5804 - val_accuracy: 0.1401\n",
      "Epoch 34/80\n",
      "542/542 [==============================] - 37s 69ms/step - loss: 4.0190 - accuracy: 0.2968 - val_loss: 8.6815 - val_accuracy: 0.1402\n",
      "Epoch 35/80\n",
      "542/542 [==============================] - 38s 71ms/step - loss: 3.9086 - accuracy: 0.3070 - val_loss: 8.7768 - val_accuracy: 0.1401\n",
      "Epoch 36/80\n",
      "542/542 [==============================] - 39s 72ms/step - loss: 3.8006 - accuracy: 0.3220 - val_loss: 8.8659 - val_accuracy: 0.1407\n",
      "Epoch 37/80\n",
      "542/542 [==============================] - 49s 91ms/step - loss: 3.6960 - accuracy: 0.3382 - val_loss: 8.9601 - val_accuracy: 0.1411\n",
      "Epoch 38/80\n",
      "542/542 [==============================] - 30s 55ms/step - loss: 3.5941 - accuracy: 0.3538 - val_loss: 9.0479 - val_accuracy: 0.1396\n",
      "Epoch 39/80\n",
      "542/542 [==============================] - 43s 79ms/step - loss: 3.4961 - accuracy: 0.3715 - val_loss: 9.1413 - val_accuracy: 0.1399\n",
      "Epoch 40/80\n",
      "542/542 [==============================] - 38s 70ms/step - loss: 3.4020 - accuracy: 0.3891 - val_loss: 9.2271 - val_accuracy: 0.1379\n",
      "Epoch 41/80\n",
      "542/542 [==============================] - 43s 79ms/step - loss: 3.3118 - accuracy: 0.4027 - val_loss: 9.3153 - val_accuracy: 0.1389\n",
      "Epoch 42/80\n",
      "542/542 [==============================] - 39s 72ms/step - loss: 3.2259 - accuracy: 0.4174 - val_loss: 9.3978 - val_accuracy: 0.1389\n",
      "Epoch 43/80\n",
      "542/542 [==============================] - 38s 70ms/step - loss: 3.1435 - accuracy: 0.4295 - val_loss: 9.4712 - val_accuracy: 0.1392\n",
      "Epoch 44/80\n",
      "542/542 [==============================] - 38s 71ms/step - loss: 3.0647 - accuracy: 0.4420 - val_loss: 9.5423 - val_accuracy: 0.1379\n",
      "Epoch 45/80\n",
      "542/542 [==============================] - 39s 73ms/step - loss: 2.9899 - accuracy: 0.4536 - val_loss: 9.6213 - val_accuracy: 0.1376\n",
      "Epoch 46/80\n",
      "542/542 [==============================] - 39s 71ms/step - loss: 2.9180 - accuracy: 0.4641 - val_loss: 9.6890 - val_accuracy: 0.1354\n",
      "Epoch 47/80\n",
      "542/542 [==============================] - 34s 63ms/step - loss: 2.8492 - accuracy: 0.4739 - val_loss: 9.7661 - val_accuracy: 0.1364\n",
      "Epoch 48/80\n",
      "542/542 [==============================] - 35s 65ms/step - loss: 2.7832 - accuracy: 0.4844 - val_loss: 9.8179 - val_accuracy: 0.1369\n",
      "Epoch 49/80\n",
      "542/542 [==============================] - 35s 65ms/step - loss: 2.7197 - accuracy: 0.4947 - val_loss: 9.8840 - val_accuracy: 0.1366\n",
      "Epoch 50/80\n",
      "542/542 [==============================] - 35s 64ms/step - loss: 2.6586 - accuracy: 0.5032 - val_loss: 9.9484 - val_accuracy: 0.1376\n",
      "Epoch 51/80\n",
      "542/542 [==============================] - 36s 66ms/step - loss: 2.5997 - accuracy: 0.5116 - val_loss: 10.0161 - val_accuracy: 0.1361\n",
      "Epoch 52/80\n",
      "542/542 [==============================] - 37s 68ms/step - loss: 2.5426 - accuracy: 0.5207 - val_loss: 10.0651 - val_accuracy: 0.1347\n",
      "Epoch 53/80\n",
      "542/542 [==============================] - 35s 64ms/step - loss: 2.4871 - accuracy: 0.5297 - val_loss: 10.1215 - val_accuracy: 0.1346\n",
      "Epoch 54/80\n",
      "542/542 [==============================] - 35s 65ms/step - loss: 2.4339 - accuracy: 0.5383 - val_loss: 10.1818 - val_accuracy: 0.1331\n",
      "Epoch 55/80\n",
      "542/542 [==============================] - 37s 67ms/step - loss: 2.3823 - accuracy: 0.5472 - val_loss: 10.2440 - val_accuracy: 0.1327\n",
      "Epoch 56/80\n",
      "542/542 [==============================] - 43s 80ms/step - loss: 2.3324 - accuracy: 0.5547 - val_loss: 10.2969 - val_accuracy: 0.1341\n",
      "Epoch 57/80\n",
      "542/542 [==============================] - 37s 69ms/step - loss: 2.2837 - accuracy: 0.5629 - val_loss: 10.3470 - val_accuracy: 0.1317\n",
      "Epoch 58/80\n",
      "542/542 [==============================] - 37s 68ms/step - loss: 2.2366 - accuracy: 0.5705 - val_loss: 10.4133 - val_accuracy: 0.1342\n",
      "Epoch 59/80\n",
      "542/542 [==============================] - 38s 70ms/step - loss: 2.1909 - accuracy: 0.5778 - val_loss: 10.4633 - val_accuracy: 0.1342\n",
      "Epoch 60/80\n",
      "542/542 [==============================] - 37s 68ms/step - loss: 2.1463 - accuracy: 0.5857 - val_loss: 10.5161 - val_accuracy: 0.1339\n",
      "Epoch 61/80\n",
      "542/542 [==============================] - 41s 75ms/step - loss: 2.1033 - accuracy: 0.5943 - val_loss: 10.5683 - val_accuracy: 0.1321\n",
      "Epoch 62/80\n",
      "542/542 [==============================] - 38s 71ms/step - loss: 2.0608 - accuracy: 0.6021 - val_loss: 10.6318 - val_accuracy: 0.1337\n",
      "Epoch 63/80\n",
      "542/542 [==============================] - 38s 69ms/step - loss: 2.0207 - accuracy: 0.6095 - val_loss: 10.6850 - val_accuracy: 0.1331\n",
      "Epoch 64/80\n",
      "542/542 [==============================] - 39s 71ms/step - loss: 1.9811 - accuracy: 0.6154 - val_loss: 10.7314 - val_accuracy: 0.1324\n",
      "Epoch 65/80\n",
      "542/542 [==============================] - 41s 75ms/step - loss: 1.9422 - accuracy: 0.6246 - val_loss: 10.7866 - val_accuracy: 0.1327\n",
      "Epoch 66/80\n",
      "542/542 [==============================] - 37s 68ms/step - loss: 1.9045 - accuracy: 0.6288 - val_loss: 10.8362 - val_accuracy: 0.1322\n",
      "Epoch 67/80\n",
      "542/542 [==============================] - 39s 73ms/step - loss: 1.8687 - accuracy: 0.6372 - val_loss: 10.8847 - val_accuracy: 0.1319\n",
      "Epoch 68/80\n",
      "542/542 [==============================] - 38s 70ms/step - loss: 1.8329 - accuracy: 0.6438 - val_loss: 10.9379 - val_accuracy: 0.1316\n",
      "Epoch 69/80\n",
      "542/542 [==============================] - 37s 68ms/step - loss: 1.7985 - accuracy: 0.6492 - val_loss: 10.9910 - val_accuracy: 0.1332\n",
      "Epoch 70/80\n",
      "542/542 [==============================] - 37s 68ms/step - loss: 1.7648 - accuracy: 0.6549 - val_loss: 11.0480 - val_accuracy: 0.1311\n",
      "Epoch 71/80\n",
      "542/542 [==============================] - 38s 70ms/step - loss: 1.7322 - accuracy: 0.6615 - val_loss: 11.1073 - val_accuracy: 0.1322\n",
      "Epoch 72/80\n",
      "542/542 [==============================] - 37s 68ms/step - loss: 1.6999 - accuracy: 0.6683 - val_loss: 11.1513 - val_accuracy: 0.1307\n",
      "Epoch 73/80\n",
      "542/542 [==============================] - 37s 69ms/step - loss: 1.6693 - accuracy: 0.6739 - val_loss: 11.1992 - val_accuracy: 0.1309\n",
      "Epoch 74/80\n",
      "542/542 [==============================] - 38s 70ms/step - loss: 1.6393 - accuracy: 0.6795 - val_loss: 11.2637 - val_accuracy: 0.1317\n",
      "Epoch 75/80\n",
      "542/542 [==============================] - 38s 70ms/step - loss: 1.6102 - accuracy: 0.6845 - val_loss: 11.3031 - val_accuracy: 0.1306\n",
      "Epoch 76/80\n",
      "542/542 [==============================] - 37s 69ms/step - loss: 1.5816 - accuracy: 0.6898 - val_loss: 11.3613 - val_accuracy: 0.1292\n",
      "Epoch 77/80\n",
      "542/542 [==============================] - 38s 70ms/step - loss: 1.5534 - accuracy: 0.6949 - val_loss: 11.4154 - val_accuracy: 0.1284\n",
      "Epoch 78/80\n",
      "542/542 [==============================] - 36s 66ms/step - loss: 1.5261 - accuracy: 0.7008 - val_loss: 11.4598 - val_accuracy: 0.1289\n",
      "Epoch 79/80\n",
      "542/542 [==============================] - 34s 63ms/step - loss: 1.5003 - accuracy: 0.7064 - val_loss: 11.5129 - val_accuracy: 0.1292\n",
      "Epoch 80/80\n",
      "542/542 [==============================] - 42s 78ms/step - loss: 1.4737 - accuracy: 0.7108 - val_loss: 11.5582 - val_accuracy: 0.1294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x221d7332188>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X, \n",
    "    y, \n",
    "    epochs=80, \n",
    "    batch_size=100,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ff3a98",
   "metadata": {},
   "source": [
    "The archaelogists found documents with the following sentences: \n",
    "- `the muhammad ___ dynasty remained`\n",
    "- `because the ___ empire was`\n",
    "- `egypt and ___ formed a`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d822e1",
   "metadata": {},
   "source": [
    "Using the trained machine learning model, try to predict the center words above and complete the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2539a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_center_word(sentence):\n",
    "    \n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    features = np.zeros(len(vocab))\n",
    "    \n",
    "    for token in tokens:\n",
    "        features += vocab_vectors[token]\n",
    "    \n",
    "    features = features / 4\n",
    "    \n",
    "    # Predict center word using model\n",
    "    predicted_array = model.predict(features.reshape(1,-1))\n",
    "    max_index = np.argmax(predicted_array)\n",
    "    \n",
    "    # Target word vector\n",
    "    target = np.zeros(len(vocab))\n",
    "    target[max_index] = 1\n",
    "    \n",
    "    # Check word that matches the array\n",
    "    word = [k for k,v in vocab_vectors.items() if np.array_equal(v, target)]\n",
    "    \n",
    "    return ' '.join(tokens[0:2]+word+tokens[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ee3a74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the muhammad ali dynasty remained'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_center_word('the muhammad dynasty remained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2990ca98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'because the roman empire was'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_center_word('because the empire was')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec4fd501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'egypt and was formed a'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_center_word('egypt and formed a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5be653",
   "metadata": {},
   "source": [
    "Final task! The archaelogists want to understand which words are more similar to `egypt` (top 10) in our word vectors context. \n",
    "<br>\n",
    "<br>\n",
    "Extract the word vectors from our trained model (use any method you would like and from any layer you would want) and check which words are more similar to `egypt` using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "41009e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3479"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.index('egypt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c8b36734",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()[0]\n",
    "similarities = cosine_similarity(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "86c622ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>egypt</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>morocco</th>\n",
       "      <td>0.595999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>0.534263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>0.533961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epidemics</th>\n",
       "      <td>0.533809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunis</th>\n",
       "      <td>0.531562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>divorce</th>\n",
       "      <td>0.526187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liberalism</th>\n",
       "      <td>0.486211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autonomousnpublic</th>\n",
       "      <td>0.485979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>0.483702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0\n",
       "egypt              1.000000\n",
       "morocco            0.595999\n",
       "1912               0.534263\n",
       "1947               0.533961\n",
       "epidemics          0.533809\n",
       "tunis              0.531562\n",
       "divorce            0.526187\n",
       "liberalism         0.486211\n",
       "autonomousnpublic  0.485979\n",
       "1591               0.483702"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    similarities[3479],\n",
    "    index = vocab\n",
    ").sort_values(by=0, ascending=False).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPCourse",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
